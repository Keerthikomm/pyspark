{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvQ0MR8oMV9"
      },
      "source": [
        "# Data Engineer Test\n",
        "\n",
        "For this programming assignment, we'll be using a Jupyter notebook in Colab with spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-5SBDkvoMV9"
      },
      "source": [
        "## Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCdNFfh4oMV9"
      },
      "source": [
        "### Collinear points\n",
        "\n",
        "Definition of collinearity[1]: In geometry, collinearity of a set of points is the property of their lying on a single line. A set of points with this property is said to be collinear.\n",
        "\n",
        "![](non-collinear-points.jpg)\n",
        "\n",
        "Here, points P,Q,R and A,R,B are collinear. However, points A,B,C are non-collinear. For more, refer [2].\n",
        "\n",
        "1. https://en.wikipedia.org/wiki/Collinearity\n",
        "2. http://www.mathcaptain.com/geometry/collinear-points.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHpTzeS_oMV-"
      },
      "source": [
        "### Parameterizing lines\n",
        "In order to determine whether a set of points all lie on the same line we need a standard way to define (or parametrize) a line.\n",
        "\n",
        "* One way of defining a line is as the set of points $(x,y)$ such that $y=ax+b$ for some fixed real values $a,b$.\n",
        "* We call $a$ the **slope** of the line and $b$ is the $y$-intercept which is defined as the value of $y$ when $x=0$.\n",
        "* This parameterization works for *almost* all lines. It does not work for vertical lines. For those lines we define $a$ to be **infinity** and $b$ to be the $x$ intercept of the line (the line is parallel to the $y$ axis so it does not intercept the $y$ axis (other than if it is the vertical line going through the origin).\n",
        "\n",
        "To summarize, given two different points $(x_1,y_1) \\neq (x_2,y_2)$, we define the parameterization $(a,b)$ as:\n",
        "* **if $x_1=x_2$: ** $(\\mbox{Inf},x_1)$\n",
        "* **Else:** $(a,b)$ such that $y_1=a x_1 +b$ and $y_2=a x_2 +b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osOQRyZToMV-"
      },
      "source": [
        "## Task\n",
        "\n",
        "Given an input file with an arbitrary set of co-ordinates, your task is to use pyspark library functions and write a program in python3 to find if three or more points are collinear.\n",
        "\n",
        "For instance, if given these points: {(1,1), (0,1), (2,2), (3,3), (0,5), (3,4), (5,6), (0,-3), (-2,-2)}\n",
        "\n",
        "Sets of collinear points are: {((-2,-2), (1,1), (2,2), (3,3)), ((0,1), (3,4), (5,6)), ((0,-3), (0,1), (0,5))}. Note that the ordering of the points in a set or the order of the sets does not matter.\n",
        "\n",
        "Note:\n",
        "<ul>\n",
        "  <li>Every set of collinear points has to have <b>at least three points</b> (any pair of points lie on a line).</li>\n",
        "  <li>There are test cases that given to you as a part of the notebook. These tests will help you validate your program and figure out bugs in it if any.</li>\n",
        "  <li>Any cell that does not require you to submit code cannot be modified. For example: Assert statement unit test cells. Cells that have \"**# YOUR CODE HERE**\" are the ONLY ones you will need to alter. </li>\n",
        "  <li>DO NOT change the names of functions. </li>\n",
        "  <li>Remove the \"Raise NotImplementedError()\" line when you write the definition of your function.</li>\n",
        "      \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6Y5193oMV-"
      },
      "source": [
        "### Description of the Approach\n",
        "\n",
        "There are many ways to find sets of collinear points from a list of points. For the purposes of this assignment, we shall stick with the below approach:\n",
        "\n",
        "1. List all pairs of points. You can do that efficiently in spark by computing cartesian product of the list of points with itself. For example, given three points $[(1,0), (2,0), (3,0)]$, we construct a list of nine pairs  \n",
        "$[((1,0),(1,0)),((1,0), (2,0)),((1,0),(3,0))$  \n",
        "$((2,0),(1,0)),((2,0), (2,0)),((2,0),(3,0))$  \n",
        "$((3,0),(1,0)),((3,0), (2,0)),((3,0),(3,0))]$  \n",
        "\n",
        "2. Remove the pairs in which the same point appears twice such as $((2,0),(2,0))$. After these elimination you end up (for this example) with a list of just six pairs:  \n",
        "$[((1,0),(2,0)),((1,0),(3,0)),((2,0),(1,0)),((2,0),(3,0)),((3,0),(1,0)),((3,0),(2,0))]$\n",
        "\n",
        "2. For each pair of points, find the parameterization $(a,b)$ of the line connecting them as described above.\n",
        "\n",
        "3. Group the pairs according to their parameters. Clearly, if two pairs have the same $(a,b)$ values, all points in the two pairs lie on the same line.\n",
        "\n",
        "3. Eliminate the groups that contain only one pair (any pair of points defines a line).\n",
        "4. In each of the remaining groups, unpack the point-pairs to identify the individual points.\n",
        "Note that if a set of points $(x_1,y_1),\\ldots,(x_k,y_k)$ lie on the same line then each point will appear $k-1$ times in the list of point-pairs. You therefore need to transform the list of points into sets to remove duplicates.\n",
        "\n",
        "5. Output the sets of 3 or more colinear points.\n",
        "\n",
        "Your task is to implement the described algorithm in Spark. You should use RDD's all the way through and collect the results into the driver only at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u88sLCYoMV-"
      },
      "source": [
        "### Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following instructions setup spark enviorenment on google Colab shared runtime. If you want you can use Local runtime on your PC and connect to Colab. You can find the instructions [here](https://research.google.com/colaboratory/local-runtimes.html), all the setup eventual issues are up to you."
      ],
      "metadata": {
        "id": "-g1ORXfgqtJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "!pip install pyspark==3.4.0\n",
        "# Spark SQL\n",
        "!pip install pyspark[sql]==3.4.0\n",
        "# pandas API on Spark\n",
        "!pip install pyspark[pandas_on_spark]==3.4.2 plotly  # to plot your data, you can install plotly together.\n",
        "# Spark Connect\n",
        "!pip install pyspark[connect]==3.4.0\n",
        "# Set environment variables\n"
      ],
      "metadata": {
        "id": "HcY2WAHUo6yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fb0040-80ed-49ea-c319-88afff79330e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.4.2\n",
            "    Uninstalling pyspark-3.4.2:\n",
            "      Successfully uninstalled pyspark-3.4.2\n",
            "Successfully installed pyspark-3.4.0\n",
            "Requirement already satisfied: pyspark[sql]==3.4.0 in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark[sql]==3.4.0) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pyspark[sql]==3.4.0) (2.0.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyspark[sql]==3.4.0) (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pyspark[sql]==3.4.0) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[sql]==3.4.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[sql]==3.4.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[sql]==3.4.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[sql]==3.4.0) (1.16.0)\n",
            "Collecting pyspark[pandas_on_spark]==3.4.2\n",
            "  Using cached pyspark-3.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]==3.4.2) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]==3.4.2) (2.0.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]==3.4.2) (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pyspark[pandas_on_spark]==3.4.2) (1.25.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]==3.4.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]==3.4.2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[pandas_on_spark]==3.4.2) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[pandas_on_spark]==3.4.2) (1.16.0)\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.4.0\n",
            "    Uninstalling pyspark-3.4.0:\n",
            "      Successfully uninstalled pyspark-3.4.0\n",
            "Successfully installed pyspark-3.4.2\n",
            "Collecting pyspark[connect]==3.4.0\n",
            "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (2.0.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (14.0.2)\n",
            "Requirement already satisfied: grpcio>=1.48.1 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status>=1.48.1 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (1.48.2)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.56.4 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (1.63.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from pyspark[connect]==3.4.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from googleapis-common-protos>=1.56.4->pyspark[connect]==3.4.0) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[connect]==3.4.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[connect]==3.4.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->pyspark[connect]==3.4.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect]==3.4.0) (1.16.0)\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.4.2\n",
            "    Uninstalling pyspark-3.4.2:\n",
            "      Successfully uninstalled pyspark-3.4.2\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Spark Context for excercises"
      ],
      "metadata": {
        "id": "dbGpdPRFrgrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zpsb_ljIoMV-"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().setAppName(\"Collinear Points\").setMaster(\"local[4]\") #Initialize spark context using 4 local cores as workers\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "from pyspark.rdd import RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recHRU1EoMV-"
      },
      "source": [
        "### Helper Functions\n",
        "Here are some helper functions that you are encouraged to use in your implementations. Do not change these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWxPC_VvoMV-"
      },
      "source": [
        "The function <font color=\"blue\">format_result</font> takes an element of the form shown below in the example. It outputs a tuple of all points that are collinear (shown below).\n",
        "\n",
        "Input: ((A,slope), [C1,..., Ck]) where each of A, C1, ..., Ck is a point of form (Ax, Ay) and slope is of type float.\n",
        "\n",
        "**<font color=\"magenta\" size=2>Example Code</font>**\n",
        "``` python\n",
        "my_input = (((2, 1), 0.5), [(4, 2), (6, 3)])\n",
        "format_result(my_input)\n",
        "```\n",
        "Output: (C1,..., Ck, A) each of A,C1,...,Ck is a point of form (Ax, Ay)\n",
        "\n",
        "**<font color=\"blue\" size=2>Example Output</font>**\n",
        "``` python\n",
        "((4, 2), (6, 3), (2, 1))\n",
        "```\n",
        "\n",
        "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlZLGKszoMV_"
      },
      "outputs": [],
      "source": [
        "def format_result(x):\n",
        "    return tuple(x[1]+[x[0][0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QZGQBONxoMV_"
      },
      "outputs": [],
      "source": [
        "def to_sorted_points(x):\n",
        "    \"\"\"\n",
        "    Sorts and returns a tuple of points for further processing.\n",
        "    \"\"\"\n",
        "    return tuple(sorted(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_H0tlLQoMV_"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "Here are some functions that you will implement. You should follow the function definitions, and use them appropriately elsewhere in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZ4NRjuoMV_"
      },
      "source": [
        "### Exercise 1: to_tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaoIRoQmoMV_"
      },
      "source": [
        "#### Example\n",
        "The function <font color=\"blue\">to_tuple</font> converts each point of form 'Ax Ay' into a point of form (Ax, Ay) for further processing.\n",
        "\n",
        "**<font color=\"magenta\" size=2>Example Code</font>**\n",
        "``` python\n",
        "my_input = '2 3'\n",
        "to_tuple(my_input)\n",
        "```\n",
        "\n",
        "**<font color=\"blue\" size=2>Example Output</font>**\n",
        "``` python\n",
        "(2, 3)\n",
        "```\n",
        "\n",
        "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HANiNmyOoMV_"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RDxt3te0oMV_"
      },
      "outputs": [],
      "source": [
        "def to_tuple(x):\n",
        "    t = x.split()\n",
        "    return(int(t[0]), int(t[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDRY2uPLoMV_"
      },
      "source": [
        "#### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xaRyOkBUoMV_"
      },
      "outputs": [],
      "source": [
        "assert type(to_tuple('1 1')) == tuple, \"Incorrect type: Element returned is not a tuple\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VqJXKEWvoMV_"
      },
      "outputs": [],
      "source": [
        "assert type(to_tuple('1 1')[0])==int and type(to_tuple('1 1')[1])==int, \"Incorrect element type: Element returned is not an integer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YtsHEZMIoMV_"
      },
      "outputs": [],
      "source": [
        "assert to_tuple('1 1') == (1,1), \"Incorrect Return Value: Value obtained does not match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YScbSOkfoMV_"
      },
      "source": [
        "### Exercise 2: non_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoXCMG0FoMV_"
      },
      "source": [
        "#### Example\n",
        "\n",
        "The function <font color=\"blue\">non_duplicates</font> checks if a set of points contains duplicates or not.\n",
        "\n",
        "Input: Pair (A,B) where A and B are of form (Ax, Ay) and (Bx, By) respectively.\n",
        "\n",
        "**<font color=\"magenta\" size=2>Example Code</font>**\n",
        "``` python\n",
        "my_input = ((0,0),(1,2))\n",
        "non_duplicates(my_input)\n",
        "```\n",
        "\n",
        "Output: Returns True if A != B, False otherwise.\n",
        "\n",
        "**<font color=\"blue\" size=2>Example Output</font>**\n",
        "``` python\n",
        "True\n",
        "```\n",
        "\n",
        "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJTp-p9loMV_"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xc6wriIuoMV_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def non_duplicates(x):\n",
        "    \"\"\"\n",
        "    Use this function inside the get_cartesian() function to 'filter' out pairs with duplicate points\n",
        "    \"\"\"\n",
        "    return x[0] != x[1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY_tOuqvoMV_"
      },
      "source": [
        "#### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pgPRDcN0oMV_"
      },
      "outputs": [],
      "source": [
        "assert type(non_duplicates(((0,0),(1,2)))) == bool, \"Incorrect Return type: Function should return a boolean value\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ArLZ0GZNoMV_"
      },
      "outputs": [],
      "source": [
        "assert non_duplicates(((0,0),(1,2))) == True, \"No duplicates are present\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DYlB_0oIoMV_"
      },
      "outputs": [],
      "source": [
        "assert non_duplicates(((0,0),(0,0))) == False, \"Duplicates exist: (0,0)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_qFA14GoMV_"
      },
      "source": [
        "### Exercise 3: get_cartesian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPKcno-toMV_"
      },
      "source": [
        "#### Example\n",
        "\n",
        "The function <font color=\"blue\">get_cartesian</font> does a cartesian product of an RDD with itself and returns an RDD with <b>DISTINCT</b> pairs of points.\n",
        "\n",
        "Input: An RDD containing the given list of points\n",
        "\n",
        "Output: An RDD containing The cartesian product of the RDD with itself\n",
        "\n",
        "**<font color=\"magenta\" size=2>Example Code</font>**\n",
        "``` python\n",
        "test_rdd = sc.parallelize([(1,0), (2,0), (3,0)])\n",
        "get_cartesian(test_rdd).collect()\n",
        "```\n",
        "\n",
        "**<font color=\"blue\" size=2>Example Output</font>**\n",
        "``` python\n",
        "[((1, 0), (2, 0)), ((1, 0), (3, 0)), ((2, 0), (1, 0)), ((2, 0), (3, 0)), ((3, 0), (1, 0)), ((3, 0), (2, 0))]\n",
        "```\n",
        "\n",
        "Refer:  http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=cartesian#pyspark.RDD.cartesian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCOefWlwoMWA"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7r_IO3_UoMWA"
      },
      "outputs": [],
      "source": [
        "def get_cartesian(rdd):\n",
        "    # YOUR CODE HERE\n",
        "    cp =  rdd.cartesian(rdd)\n",
        "    distinct_cp = cp.filter(lambda x: non_duplicates(x))\n",
        "    return distinct_cp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp-z81eZoMWA"
      },
      "source": [
        "#### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iSrjiubQoMWA"
      },
      "outputs": [],
      "source": [
        "test_rdd = sc.parallelize([(1,0), (2,0), (3,0)])\n",
        "\n",
        "l = [((1, 0), (2, 0)), ((1, 0), (3, 0)), ((2, 0), (1, 0)), ((2, 0), (3, 0)), ((3, 0), (1, 0)), ((3, 0), (2, 0))]\n",
        "\n",
        "assert isinstance(get_cartesian(test_rdd), RDD) == True, \"Incorrect Return type: Function should return an RDD\"\n",
        "assert set(get_cartesian(test_rdd).collect()) == set(l), \"Incorrect Return Value: Value obtained does not match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSI0lsYBoMWA"
      },
      "source": [
        "### Exercise 4: find_slope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk7hs45HoMWA"
      },
      "source": [
        "#### Example\n",
        "\n",
        "The function <font color=\"blue\">find_slope</font> computes slope between points A and B and returns it in the format specified below.\n",
        "\n",
        "Input: Pair (A,B) where A and B are of form (Ax, Ay) and (Bx, By) respectively.\n",
        "\n",
        "**<font color=\"magenta\" size=2>Example Code</font>**\n",
        "``` python\n",
        "my_input = ((1,2),(3,4))\n",
        "find_slope(my_input)\n",
        "```\n",
        "\n",
        "Output: Pair ((A,slope), B) where A and B have the same definition as input and slope refers to the slope of the line segment connecting point A and B.\n",
        "\n",
        "**<font color=\"blue\" size=2>Example Output</font>**\n",
        "``` python\n",
        "(((1, 2), 1.0), (3, 4))\n",
        "```\n",
        "<font color=\"brown\">**Note: **</font> If Ax == Bx, use slope as \"inf\".\n",
        "\n",
        "<font color=\"red\">**Hint : **</font> The above example is given just to provide the input and output format. This function is called a different way in the spark exercise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8HJzJ5XoMWA"
      },
      "source": [
        "#### Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G9xWRlFxoMWA"
      },
      "outputs": [],
      "source": [
        "def find_slope(x):\n",
        "    # YOUR CODE HERE\n",
        "    A, B = x\n",
        "    Ax, Ay = A\n",
        "    Bx, By = B\n",
        "\n",
        "    if Ax == Bx:\n",
        "        slope = \"inf\"\n",
        "    else:\n",
        "        slope = (By - Ay) / (Bx - Ax)\n",
        "\n",
        "    return ((A, slope), B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyAlulFjoMWD"
      },
      "source": [
        "#### Unit tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-mKmeeGIoMWD"
      },
      "outputs": [],
      "source": [
        "assert type(find_slope(((1,2),(3,4)))) == tuple, \"Function must return a tuple\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R6DM3stKoMWD"
      },
      "outputs": [],
      "source": [
        "assert find_slope(((1,2),(-7,-2)))[0][1] == 0.5, \"Slope value should be 0.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rsXsE5kmoMWD"
      },
      "outputs": [],
      "source": [
        "assert find_slope(((1,2),(3,4))) == (((1,2),1),(3,4)), \"Incorrect return value: Value obtained does not match\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Evo5AwURoMWD"
      },
      "outputs": [],
      "source": [
        "assert find_slope(((1,2),(1,5))) == (((1,2),\"inf\"),(1,5)), \"Incorrect return value: Value obtained must have slope 'inf'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QTToZuhvoMWD"
      },
      "outputs": [],
      "source": [
        "assert find_slope(((1,2),(2,5))) == (((1,2),3),(2,5)), \"Incorrect return value: Value obtained does not match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp3kypgYoMWD"
      },
      "source": [
        "### Exercise 5: find_collinear\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa52bTMIoMWD"
      },
      "source": [
        "#### Example\n",
        "\n",
        "The function <font color=\"blue\">find_collinear</font> finds the set of collinear points.\n",
        "\n",
        "Input: An RDD (which is the output of the get_cartesian() function.\n",
        "\n",
        "Output: An RDD containing the list of collinear points formatted according to the <font color=\"blue\">format_result</font> function.\n",
        "\n",
        "Approach:\n",
        "1. Find the slope of the line between all pairs of points A = (Ax, Ay) and B = (Bx, By).\n",
        "2. For each (A, B), find all points C = ((C1x, C1y), (C2x, C2y), ... (Cnx, Cny))\n",
        "   where slope of (A,B) = slope of (A, Ci).\n",
        "3. Return (A, B, Ck) where Ck = all points of C which satisfy the condition 1.\n",
        "\n",
        "The assert statement unit tests for this function will help you with this.\n",
        "<font color=\"red\">**Hint : **</font>   You should use the above helper functions in conjunction with Spark RDD API (refer http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD)\n",
        "            Finally, use helper function format_result() appropriately from inside this function after you have implemented the above operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpYvYxenoMWD"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6IrQwc6ioMWD"
      },
      "outputs": [],
      "source": [
        "def find_collinear(rdd):\n",
        "    # YOUR CODE HERE\n",
        "    return rdd.map(find_slope).groupByKey().map(lambda x:(x[0][0],) + tuple(x[1])).filter(lambda x: len(x) > 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdW34EhBoMWD"
      },
      "source": [
        "#### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bdUjs5aOoMWD"
      },
      "outputs": [],
      "source": [
        "def verify_collinear_sets(collinearpointsRDD, testlist):\n",
        "    collinearpoints = [tuple(sorted(x)) for x in list(set(collinearpointsRDD.collect()))]\n",
        "    testlist = [tuple(sorted(x)) for x in list(set(testlist))]\n",
        "    return set(collinearpoints) == set(testlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gd8b27FqoMWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822cd315-a8b3-4e4c-fc03-0772414c73c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((2, 1), (4, 2), (6, 3)), ((4, 2), (2, 1), (6, 3)), ((6, 3), (4, 2), (2, 1))]\n"
          ]
        }
      ],
      "source": [
        "test_rdd = sc.parallelize([((4, 2), (2, 1)), ((4, 2), (-3, 4)), ((4, 2), (6, 3)), ((2, 1), (4, 2)), ((2, 1), (-3, 4)), ((2, 1), (6, 3)), ((-3, 4), (4, 2)), ((-3, 4), (2, 1)), ((-3, 4), (6, 3)), ((6, 3), (4, 2)), ((6, 3), (2, 1)), ((6, 3), (-3, 4))])\n",
        "print(find_collinear(test_rdd).collect())\n",
        "assert isinstance(find_collinear(test_rdd), RDD) == True, \"Incorrect return type: Function must return RDD\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jK_jySzVoMWE"
      },
      "outputs": [],
      "source": [
        "assert verify_collinear_sets(find_collinear(test_rdd), [((2, 1), (4, 2), (6, 3))]), \"Incorrect return value: Value obtained does not match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFCY-wFooMWE"
      },
      "source": [
        "#### Unit Tests II : Using the output of get_cartesian(rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BtYS0T-1oMWE"
      },
      "outputs": [],
      "source": [
        "test_rdd = sc.parallelize([(4, -2), (2, -1), (-3,4), (6,3), (-9,4), (6, -3), (8,-4), (6,9)])\n",
        "test_rdd = get_cartesian(test_rdd)\n",
        "assert verify_collinear_sets(find_collinear(test_rdd), [((6, -3), (6, 3), (6, 9)), ((2, -1), (4, -2), (6, -3), (8, -4))]), \"Incorrect return value: You have not implemented the find_collinear function in Python\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f43ZGevLoMWE"
      },
      "source": [
        "### Exercise 6: The build_collinear_set function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN_GJLcCoMWE"
      },
      "source": [
        "#### Example\n",
        "Using the above functions that you have written along with pyspark functions, write the **build_collinear_set** function and returns an RDD containing the set of collinear points.\n",
        "\n",
        "Input: RDD containing the given set of points\n",
        "\n",
        "Output: RDD containing the set of collinear points\n",
        "\n",
        "<font color=\"red\">**Hint : **</font> Remember that the input RDD consists of a set of strings. Remember to pre-process them using the to_tuple function before performing other operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxfRMrC1oMWE"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W83Rvy7CoMWE"
      },
      "outputs": [],
      "source": [
        "def build_collinear_set(rdd):\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    cls=rdd.map(lambda x: to_tuple(x))\n",
        "    cls=get_cartesian(cls)\n",
        "    rdd=find_collinear(cls)\n",
        "\n",
        "    # Sorting each of your returned sets of collinear points.\n",
        "    rdd = rdd.map(to_sorted_points)\n",
        "\n",
        "    return rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rioZSOIloMWE"
      },
      "source": [
        "#### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "THBSauqkoMWE"
      },
      "outputs": [],
      "source": [
        "test_rdd = sc.parallelize(['4 -2', '2 -1', '-3 4', '6 3', '-9 4', '6 -3', '8 -4', '6 9'])\n",
        "assert isinstance(build_collinear_set(test_rdd), RDD) == True, \"build_collinear_set should return an RDD.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIYkNzxWoMWF"
      },
      "source": [
        "### The process function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFTOaJTkoMWF"
      },
      "source": [
        "#### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gEgBuYfNoMWF"
      },
      "outputs": [],
      "source": [
        "def process(filename):\n",
        "    \"\"\"\n",
        "    This is the process function used for finding collinear points using inputs from different files\n",
        "    Input: Name of the test file\n",
        "    Output: Set of collinear points\n",
        "    \"\"\"\n",
        "    # Load the data file into an RDD\n",
        "    rdd = sc.textFile(filename)\n",
        "    # YOUR CODE HERE\n",
        "    rdd = build_collinear_set(rdd)\n",
        "\n",
        "    # Collecting the collinear points RDD in a set to remove duplicate sets of collinear points.\n",
        "    res = set(rdd.collect())\n",
        "    # YOUR CODE HERE\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIIYwKGGoMWF"
      },
      "source": [
        "#### Unit Tests: Testing the build_collinear_set function using the process function\n",
        "NOTE: You may assume that input files do not have duplicate points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7lO0gCW1oMWF"
      },
      "outputs": [],
      "source": [
        "assert process(\"/content/sample_data/data.txt\") == {((-2, -2), (1, 1), (2, 2), (3, 3)), ((0, 1), (3, 4), (5, 6)), ((0, -3), (0, 1), (0, 5))}, \"Your implementation of build_collinear_set is not correct.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NiUULWNloMWF"
      },
      "outputs": [],
      "source": [
        "assert process(\"data50.txt\") == {((3, 6), (7, 4), (9, 3)), ((1, 6), (3, 6), (4, 6), (7, 6)),\n",
        "                                 ((0, 2), (3, 1), (6, 0)), ((1, 0), (2, 0), (5, 0), (6, 0)),\n",
        "                                 ((1, 3), (3, 6), (5, 9)), ((0, 8), (4, 6), (6, 5)),\n",
        "                                 ((6, 0), (6, 1), (6, 5), (6, 9)),\n",
        "                                 ((7, 2), (7, 3), (7, 4), (7, 6), (7, 8)), ((3, 1), (3, 3), (3, 6)),\n",
        "                                 ((0, 2), (1, 2), (5, 2), (7, 2)), ((0, 3), (2, 5), (3, 6), (6, 9)),\n",
        "                                 ((0, 2), (1, 3), (2, 4), (4, 6), (5, 7)), ((1, 2), (4, 3), (7, 4)),\n",
        "                                 ((0, 3), (4, 6), (8, 9)), ((9, 3), (9, 4), (9, 5)), ((2, 5), (5, 7), (8, 9)),\n",
        "                                 ((0, 5), (2, 4), (4, 3), (8, 1)), ((0, 8), (1, 6), (2, 4)),\n",
        "                                 ((3, 6), (5, 2), (6, 0)), ((5, 9), (6, 9), (8, 9)),\n",
        "                                 ((0, 8), (1, 8), (7, 8)), ((0, 4), (1, 3), (3, 1)), ((5, 9), (7, 6), (9, 3)),\n",
        "                                 ((1, 2), (2, 4), (3, 6)), ((0, 7), (1, 5), (3, 1)),\n",
        "                                 ((1, 5), (2, 4), (3, 3), (6, 0)), ((0, 2), (3, 3), (9, 5)),\n",
        "                                 ((0, 7), (1, 6), (2, 5), (4, 3), (5, 2), (6, 1)),\n",
        "                                 ((0, 4), (1, 5), (5, 9)), ((1, 5), (3, 6), (5, 7), (7, 8)),\n",
        "                                 ((1, 6), (3, 3), (5, 0)), ((3, 6), (4, 3), (5, 0)),\n",
        "                                 ((1, 2), (4, 5), (7, 8), (8, 9)), ((0, 2), (1, 1), (2, 0)),\n",
        "                                 ((3, 3), (4, 5), (5, 7), (6, 9)), ((0, 2), (0, 3), (0, 4), (0, 5), (0, 7), (0, 8)),\n",
        "                                 ((2, 0), (4, 3), (8, 9)), ((5, 7), (6, 5), (7, 3), (8, 1)), ((5, 0), (7, 6), (8, 9)),\n",
        "                                 ((5, 0), (6, 1), (7, 2), (9, 4)), ((0, 4), (1, 2), (2, 0)),\n",
        "                                 ((1, 1), (3, 1), (6, 1), (8, 1)), ((5, 7), (7, 6), (9, 5)), ((1, 1), (7, 4), (9, 5)),\n",
        "                                 ((0, 4), (2, 4), (7, 4), (9, 4)), ((1, 0), (3, 1), (5, 2), (7, 3), (9, 4)),\n",
        "                                 ((2, 0), (3, 3), (4, 6), (5, 9)), ((4, 3), (4, 5), (4, 6)),\n",
        "                                 ((1, 0), (4, 3), (6, 5), (7, 6)), ((0, 3), (2, 4), (4, 5)),\n",
        "                                 ((1, 6), (4, 5), (7, 4)), ((1, 0), (1, 1), (1, 2), (1, 3), (1, 5), (1, 6), (1, 8)),\n",
        "                                 ((0, 3), (1, 3), (3, 3), (4, 3), (7, 3), (9, 3)), ((0, 4), (2, 5), (4, 6)),\n",
        "                                 ((0, 7), (3, 6), (6, 5), (9, 4)), ((1, 8), (4, 6), (7, 4)),\n",
        "                                 ((0, 5), (3, 3), (6, 1)), ((1, 8), (3, 6), (4, 5), (7, 2), (8, 1)),\n",
        "                                 ((1, 2), (3, 1), (5, 0)), ((1, 1), (5, 2), (9, 3)),\n",
        "                                 ((5, 0), (5, 2), (5, 7), (5, 9)), ((0, 5), (1, 5), (2, 5), (4, 5), (6, 5), (9, 5)),\n",
        "                                 ((3, 1), (4, 5), (5, 9)), ((2, 0), (2, 4), (2, 5)), ((5, 2), (6, 5), (7, 8))}, \"Your implementation of build_collinear_set is not correct.\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}